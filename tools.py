import numpy as np
import scipy.sparse as sp
import torch
import torch.nn.functional as F
from sklearn.neighbors import kneighbors_graph

# import dgl
from sklearn import metrics
from munkres import Munkres

EOS = 1e-10


def knn_fast(X, k, b):

    X = F.normalize(X, dim=1, p=2)
    index = 0
    values = torch.zeros(X.shape[0] * (k + 1)).to(
        torch.device("cuda" if torch.cuda.is_available() else "cpu")
    )
    rows = torch.zeros(X.shape[0] * (k + 1)).to(
        torch.device("cuda" if torch.cuda.is_available() else "cpu")
    )
    cols = torch.zeros(X.shape[0] * (k + 1)).to(
        torch.device("cuda" if torch.cuda.is_available() else "cpu")
    )
    norm_row = torch.zeros(X.shape[0]).to(
        torch.device("cuda" if torch.cuda.is_available() else "cpu")
    )
    norm_col = torch.zeros(X.shape[0]).to(
        torch.device("cuda" if torch.cuda.is_available() else "cpu")
    )
    while index < X.shape[0]:
        if (index + b) > (X.shape[0]):
            end = X.shape[0]
        else:
            end = index + b
        sub_tensor = X[index : index + b]
        similarities = torch.mm(sub_tensor, X.t())
        vals, inds = similarities.topk(k=k + 1, dim=-1)
        values[index * (k + 1) : (end) * (k + 1)] = vals.view(-1)
        cols[index * (k + 1) : (end) * (k + 1)] = inds.view(-1)
        rows[index * (k + 1) : (end) * (k + 1)] = (
            torch.arange(index, end).view(-1, 1).repeat(1, k + 1).view(-1)
        )
        norm_row[index:end] = torch.sum(vals, dim=1)
        norm_col.index_add_(-1, inds.view(-1), vals.view(-1))
        index += b
    norm = norm_row + norm_col
    rows = rows.long()
    cols = cols.long()
    values *= torch.pow(norm[rows], -0.5) * torch.pow(norm[cols], -0.5)
    return rows, cols, values


def apply_non_linearity(tensor, non_linearity, i):
    if non_linearity == "elu":
        return F.elu(tensor * i - i) + 1
    elif non_linearity == "relu":
        return F.relu(tensor)
    elif non_linearity == "none":
        return tensor
    else:
        raise NameError("We dont support the non-linearity yet")


def symmetrize(adj):
    return (adj + adj.T) / 2


def cal_similarity_graph(node_embeddings):
    similarity_graph = torch.mm(node_embeddings, node_embeddings.t())
    return similarity_graph


def top_k(raw_graph, K):
    values, indices = raw_graph.topk(k=int(K), dim=-1)
    assert torch.max(indices) < raw_graph.shape[1]
    mask = torch.zeros(raw_graph.shape).to(
        torch.device("cuda" if torch.cuda.is_available() else "cpu")
    )
    mask[torch.arange(raw_graph.shape[0]).view(-1, 1), indices] = 1.0

    mask.requires_grad = False
    sparse_graph = raw_graph * mask
    return sparse_graph


def normalize(adj, mode, sparse=False):
    if not sparse:
        if mode == "sym":
            inv_sqrt_degree = 1.0 / (torch.sqrt(adj.sum(dim=1, keepdim=False)) + EOS)
            return inv_sqrt_degree[:, None] * adj * inv_sqrt_degree[None, :]
        elif mode == "row":
            inv_degree = 1.0 / (adj.sum(dim=1, keepdim=False) + EOS)
            return inv_degree[:, None] * adj
        else:
            exit("wrong norm mode")
    else:
        adj = adj.coalesce()
        if mode == "sym":
            inv_sqrt_degree = 1.0 / (
                torch.sqrt(torch.sparse.sum(adj, dim=1).values()) + EOS
            )
            D_value = (
                inv_sqrt_degree[adj.indices()[0]] * inv_sqrt_degree[adj.indices()[1]]
            )

        elif mode == "row":
            aa = torch.sparse.sum(adj, dim=1)
            bb = aa.values()
            inv_degree = 1.0 / (torch.sparse.sum(adj, dim=1).values() + EOS)
            D_value = inv_degree[adj.indices()[0]]
        else:
            exit("wrong norm mode")
        new_values = adj.values() * D_value

        return torch.sparse.FloatTensor(
            adj.indices(), new_values, adj.size()
        ).coalesce()


def sim_con(z1, z2, temperature):

    z1_norm = torch.norm(z1, dim=-1, keepdim=True)
    z2_norm = torch.norm(z2, dim=-1, keepdim=True)
    dot_numerator = torch.mm(z1, z2.t())
    dot_denominator = torch.mm(z1_norm, z2_norm.t()) + EOS
    sim_matrix = dot_numerator / dot_denominator / temperature
    return sim_matrix


def dense_to_sparse(dense_matrix):

    if not isinstance(dense_matrix, torch.Tensor):
        raise ValueError("输入必须是一个 PyTorch 张量。")

    if dense_matrix.dim() != 2:
        raise ValueError("输入的张量必须是二维的。")

    dense_np = dense_matrix.cpu().detach().numpy()
    indices = np.nonzero(dense_np)
    values = dense_np[indices]

    indices = torch.tensor(indices, dtype=torch.long)
    values = torch.tensor(values, dtype=torch.float32)

    sparse_matrix = torch.sparse.FloatTensor(indices, values, dense_matrix.size())
    return sparse_matrix
